{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c99c77f-3142-410e-8b8e-c411e945b23a",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2adaecc-70d4-4869-9256-18ce54a8019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e45b1-7c8a-40ea-82a2-96d9ad975538",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b06cbc8-c9c5-4605-8abd-6733c6375dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/Adult/train_Adult_cleaned.csv')\n",
    "labels = pd.read_csv('./data/Adult/train_Adult_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e637cae-fdc2-4543-9c54-70756754d964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>77516</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>83311</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>215646</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>234721</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>338409</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt  education  education-num  marital-status  \\\n",
       "0   39          6   77516          1             13               3   \n",
       "1   50          2   83311          1             13               1   \n",
       "2   38          1  215646          4              9               2   \n",
       "3   53          1  234721          3              7               1   \n",
       "4   28          1  338409          1             13               1   \n",
       "\n",
       "   occupation  relationship  race  sex  capital-gain  capital-loss  \\\n",
       "0           9             4     1    2          2174             0   \n",
       "1           5             3     1    2             0             0   \n",
       "2           7             4     1    2             0             0   \n",
       "3           7             3     5    2             0             0   \n",
       "4           6             1     5    1             0             0   \n",
       "\n",
       "   hours-per-week  native-country  \n",
       "0              40               1  \n",
       "1              13               1  \n",
       "2              40               1  \n",
       "3              40               1  \n",
       "4              40              13  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb90f051-a81b-4072-a77f-6bf890d6764f",
   "metadata": {},
   "source": [
    "# Save discrete columns and continuous columns separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1729b1ce-4687-4e9f-bc7e-db46096cf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "continuous_columns = [col for col in list(data.columns) if col not in discrete_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20fd362-70d2-475b-b2d2-a5fbf254acbf",
   "metadata": {},
   "source": [
    "# Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c11558b-5323-4b52-b102-35bcd253f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_clusters = min(len(data), 10)  # maximum number of clusters to consider when using Bayesian Gaussian Mixture\n",
    "weight_threshold = 0.005  # values under this threshold is considered useless in the Bayesian Gaussian Mixture\n",
    "random_state = 42\n",
    "\n",
    "SpanInfo = namedtuple('SpanInfo', ['dim', 'activation_fn'])  # make it simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d52aa1-9e19-49d9-90d8-be991a83b27e",
   "metadata": {},
   "source": [
    "# Fit and transform continuous and discrete values separately\n",
    "\n",
    "# Continuous\n",
    "- For each continuous column $C_i$ use `sklearn.mixture.BayesianGaussianMixture` to find the number of valid components. Results that are smaller than `weight_threshold` are considered useless. Determine `n_components`\n",
    "- For every value $c_{i, j}$ in the continuous column $C_i$, compute the probability of $c_{i, j}$ coming from each component. For instance, if there are 3 valid components found after BGM, compute 3 probabilities $\\rho_1, \\rho_2, \\rho_3$\n",
    "- Using the calculated probabilities, we randomly choose one component according to the probability of each component. Represent the chosen component as one-hot vector and normalize the value $c_{i, j}$ using the mean and std of that component\n",
    "- Let the normalized value be $\\alpha_{i, j}$ and the one-hot vector be $\\beta_{i, j}$. The transformed value is $\\alpha_{i, j} \\oplus \\beta_{i, j}$ ($\\oplus$ means concatenated) The shape of the result is `[1 + number of components]`\n",
    "\n",
    "# Discrete\n",
    "- Create a one hot encoding of each value\n",
    "- Shape of the result is `[number of categories]`\n",
    "\n",
    "\n",
    "The final row should look something like this:\n",
    "\n",
    "$$r_j = \\alpha_{1, j} \\oplus \\beta_{1, j} ... \\oplus \\alpha_{N_c, j} \\oplus \\beta_{N_c, 1} \\oplus d_{1, j} \\oplus ... \\oplus d_{N_d, j}$$\n",
    "\n",
    "where:\n",
    "- $N_c$ : number of continuous columns\n",
    "- $N_d$ : number of discrete columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b72d3f10-f287-4359-b0e6-80972aeae325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_transform_continuous(column_name:str, data:pd.Series, weight_threshold=0.005):\n",
    "    bgm = BayesianGaussianMixture(\n",
    "        n_components=max_clusters,\n",
    "        weight_concentration_prior_type='dirichlet_process',\n",
    "        weight_concentration_prior=0.001,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    data = data.to_numpy()\n",
    "    data = data.reshape(-1, 1)\n",
    "    # fit Bayesian Gaussian Mixture\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        bgm.fit(data)\n",
    "    valid_component_indicator = bgm.weights_ > weight_threshold  # weights close to 0 are not effective components\n",
    "    num_components = sum(valid_component_indicator)  # number of effective components (modes)\n",
    "\n",
    "    # Save the transformation information for future reference\n",
    "    transform_info = {\n",
    "        'column_name': column_name,\n",
    "        'column_type': 'continuous',\n",
    "        'transform': bgm,\n",
    "        'valid_component_indicator': valid_component_indicator,\n",
    "        'output_info': [SpanInfo(1, 'tanh'), SpanInfo(num_components, 'softmax')],\n",
    "        'output_dimensions': 1 + num_components  # 1 normalized value + number of effective gaussian mixtures\n",
    "    }\n",
    "\n",
    "    # transform\n",
    "    means = bgm.means_.reshape((1, max_clusters))  # means for each gaussian mixture\n",
    "    means = means[:, valid_component_indicator]  # only the effective ones\n",
    "    stds = np.sqrt(bgm.covariances_).reshape((1, max_clusters))  # stds for each gaussian mixture\n",
    "    stds = stds[:, valid_component_indicator]  # only the effective ones\n",
    "    \n",
    "    normalized_values = (data - means) / (4 * stds)  # normaluze the values using the components\n",
    "    component_probs = bgm.predict_proba(data)  # probability that each data belongs to a certain Gaussian Mixture\n",
    "    component_probs = component_probs[:, valid_component_indicator]  # only the effective ones\n",
    "    \n",
    "    selected_component = np.zeros(len(data), dtype=int)\n",
    "    for i in range(len(data)):\n",
    "        component_prob_t = component_probs[i] + 1e-6\n",
    "        component_prob_t /= component_prob_t.sum()  # probability of each Gaussian Mixture of ith data\n",
    "        # randomly choose one component based on the calculated probabilities\n",
    "        selected_component[i] = np.random.choice(np.arange(valid_component_indicator.sum()), p=component_prob_t)\n",
    "    \n",
    "    aranged = np.arange(len(data))\n",
    "    normalized = normalized_values[aranged, selected_component].reshape([-1, 1])  # select the normalized value of the randomly chosen mixture\n",
    "    normalized = np.clip(normalized, -0.99, 0.99)  # make normalize value fall into tanh range\n",
    "    normalized = normalized[:, 0]  # final normalized value alpha_{i, j}\n",
    "    \n",
    "    # one hot vector transform selected component according to the paper\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe.fit(selected_component.reshape(-1, 1))\n",
    "    selected_component = ohe.transform(selected_component.reshape(-1, 1)).toarray()\n",
    "    \n",
    "    row = [normalized.reshape(-1, 1), selected_component]\n",
    "    transformed = np.concatenate(row, axis=1)  # alpha_{i, j} \\oplus beta_{i, j}\n",
    "    print(f\"{column_name} shape = {transformed.shape}, num_components = {num_components}\")\n",
    "    return transformed, transform_info  # concatenation of normalized value and the selected component\n",
    "\n",
    "# one hot encode\n",
    "def fit_and_transform_discrete(column_name:str, data:pd.Series):\n",
    "    ohe = OneHotEncoder()\n",
    "    \n",
    "    data = data.to_numpy()\n",
    "    data = data.reshape(-1, 1)\n",
    "    ohe.fit(data)\n",
    "    num_categories = len(ohe.categories_[0])  # for some reason the values are wrapped in a list\n",
    "    transform_info = {\n",
    "        'column_name': column_name,\n",
    "        'column_type': 'discrete',\n",
    "        'transform': ohe,\n",
    "        'output_info': [SpanInfo(num_categories, 'softmax')],\n",
    "        'output_dimensions': num_categories  # one hot encoded\n",
    "    }\n",
    "\n",
    "    transformed = ohe.transform(data).toarray()\n",
    "    print(f\"{column_name} shape = {transformed.shape}, num_categories = {num_categories}\")\n",
    "\n",
    "    return transformed, transform_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46e5be1d-adfc-4fa0-a83c-78919aaa35c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age shape = (2000, 10), num_components = 9\n",
      "workclass shape = (2000, 8), num_categories = 8\n",
      "fnlwgt shape = (2000, 10), num_components = 9\n",
      "education shape = (2000, 16), num_categories = 16\n",
      "education-num shape = (2000, 8), num_components = 7\n",
      "marital-status shape = (2000, 6), num_categories = 6\n",
      "occupation shape = (2000, 15), num_categories = 15\n",
      "relationship shape = (2000, 6), num_categories = 6\n",
      "race shape = (2000, 5), num_categories = 5\n",
      "sex shape = (2000, 2), num_categories = 2\n",
      "capital-gain shape = (2000, 5), num_components = 4\n",
      "capital-loss shape = (2000, 3), num_components = 2\n",
      "hours-per-week shape = (2000, 10), num_components = 9\n",
      "native-country shape = (2000, 36), num_categories = 36\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "output_info_list = []\n",
    "for col in list(data.columns):\n",
    "    if col in continuous_columns:\n",
    "        continuous_data = data[col]\n",
    "        transformed, transform_info = fit_and_transform_continuous(col, continuous_data)\n",
    "        train_data.append(transformed)\n",
    "        output_info_list.append(transform_info)\n",
    "    elif col in discrete_columns:\n",
    "        discrete_data = data[col]\n",
    "        transformed, transform_info = fit_and_transform_discrete(col, discrete_data)\n",
    "        train_data.append(transformed)\n",
    "        output_info_list.append(transform_info)\n",
    "\n",
    "train_data = np.concatenate(train_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e05d4eaf-d18d-4d6d-8084-4f112eec08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_frequency = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f665335-c2d5-4644-9be2-51fef252a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b578ec8-d5e1-40d3-bac2-d61c3dc41dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_discrete_columns = len(discrete_columns)  # number of discrete columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96bd4dae-266e-4179-bd3f-45d83e70ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_column_matrix_st = np.zeros(n_discrete_columns, dtype='int32')\n",
    "rid_by_cat_cols = []  # rid_by_cat_cols[a][b] => list of all rows with the a-th discrete column equal to b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a21baae7-b9a6-4ada-a137-e239ff1703ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 0\n",
    "for column_info in output_info_list:\n",
    "    if column_info['column_name'] in discrete_columns:\n",
    "        span_info = column_info['output_info'][0]\n",
    "        ed = st + span_info.dim  # st + number of categories\n",
    "\n",
    "        rid_by_cat = []\n",
    "        for j in range(span_info.dim):\n",
    "            rid_by_cat.append(np.nonzero(train_data[:, st + j])[0])\n",
    "        rid_by_cat_cols.append(rid_by_cat)\n",
    "        st = ed\n",
    "    else:\n",
    "        st += sum([span_info.dim for span_info in column_info['output_info']])\n",
    "assert st == train_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44bc7efa-b05f-4ba4-a3f1-4fd357169255",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_category = max(\n",
    "    [column_info['output_info'][0].dim for column_info in output_info_list if column_info['column_type'] == 'discrete']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ae316cb-6e24-42c6-bbfa-74eed775aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_column_cond_st = np.zeros(n_discrete_columns, dtype='int32')\n",
    "discrete_column_n_category = np.zeros(n_discrete_columns, dtype='int32')  # number of categories of each discrete column\n",
    "discrete_column_category_prob = np.zeros((n_discrete_columns, max_category))  # probs of each category of each discrete column (PMF of each column)\n",
    "n_categories = sum([\n",
    "    column_info['output_info'][0].dim for column_info in output_info_list if column_info['column_type'] == 'discrete'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f7e6993-412a-4f4c-ab94-69c92890c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 0\n",
    "current_id = 0\n",
    "current_cond_st = 0\n",
    "for column_info in output_info_list:\n",
    "    if column_info['column_type'] == 'discrete':  # all one hot encoded\n",
    "        span_info = column_info['output_info'][0]\n",
    "        ed = st + span_info.dim\n",
    "        category_freq = np.sum(train_data[:, st:ed], axis=0)\n",
    "        if log_frequency:\n",
    "            category_freq = np.log(category_freq + 1)\n",
    "        category_prob = category_freq / np.sum(category_freq)\n",
    "        discrete_column_category_prob[current_id, :span_info.dim] = category_prob\n",
    "        discrete_column_cond_st[current_id] = current_cond_st\n",
    "        discrete_column_n_category[current_id] = span_info.dim\n",
    "        current_cond_st += span_info.dim\n",
    "        current_id += 1\n",
    "        st = ed\n",
    "    else:\n",
    "        st += sum([span_info.dim for span_info in column_info['output_info']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9396afc6-e652-4315-aafd-0d4ea4e57eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dim = train_data.shape[1]\n",
    "data_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44608bda-4f77-444f-b6dd-5634e5334e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, i, o):\n",
    "        super(Residual, self).__init__()\n",
    "        self.fc = nn.Linear(i, o)\n",
    "        self.bn = nn.BatchNorm1d(o)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        return torch.cat([out, x], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ab60065-6c0f-49bc-a9e8-8661fbb5a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, discriminator_dim, pac=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        dim = input_dim * pac\n",
    "        self.pac = pac\n",
    "        self.pacdim = dim\n",
    "        seq = []\n",
    "        for item in list(discriminator_dim):\n",
    "            seq += [nn.Linear(dim, item), nn.LeakyReLU(0.2), nn.Dropout(0.5)]\n",
    "            dim = item\n",
    "        seq += [nn.Linear(dim, 1)]\n",
    "        self.seq = nn.Sequential(*seq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.size()[0] % self.pac == 0\n",
    "        return self.seq(x.view(-1, self.pacdim))\n",
    "\n",
    "    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n",
    "        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n",
    "        alpha = alpha.repeat(1, pac, real_data.size(1))\n",
    "        alpha = alpha.view(-1, real_data.size(1))\n",
    "\n",
    "        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "        disc_interpolates = self(interpolates)\n",
    "\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=disc_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "\n",
    "        gradients_view = gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1\n",
    "        gradient_penalty = ((gradients_view) ** 2).mean() * lambda_\n",
    "\n",
    "        return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "250e0e30-dcca-4fcb-b373-eb32ef9e1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, embedding_dim, generator_dim, data_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        dim = embedding_dim\n",
    "        seq = []\n",
    "        for item in list(generator_dim):\n",
    "            seq += [Residual(dim, item)]\n",
    "            dim += item\n",
    "        seq.append(nn.Linear(dim, data_dim))\n",
    "        self.seq = nn.Sequential(*seq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        data = self.seq(x)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28b423f1-bef9-44a6-b183-233f03d6e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "033b4d15-9748-4920-a7db-b0a0a929a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "generator_dim = (256, 256)\n",
    "discriminator_dim = (256, 256)\n",
    "generator_lr = 2e-4\n",
    "generator_decay = 1e-6\n",
    "discriminator_lr = 2e-4\n",
    "discriminator_decay = 1e-6\n",
    "batch_size=500\n",
    "discriminator_step = 1\n",
    "epochs = 300\n",
    "pac = 10\n",
    "cuda = True\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b48659f-acbd-433d-bba3-4741e97dfbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(\n",
    "    embedding_dim=embedding_dim+n_categories,\n",
    "    generator_dim=generator_dim,\n",
    "    data_dim=data_dim\n",
    ").to(device)\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    input_dim=data_dim+n_categories,\n",
    "    discriminator_dim=discriminator_dim,\n",
    "    pac=pac\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "575a9f2c-003c-47e8-a3fb-6e79b14e5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerG = optim.Adam(\n",
    "    generator.parameters(),\n",
    "    lr=generator_lr,\n",
    "    betas=(0.5, 0.9),\n",
    "    weight_decay=generator_decay\n",
    ")\n",
    "\n",
    "optimizerD = optim.Adam(\n",
    "    discriminator.parameters(),\n",
    "    lr=discriminator_lr,\n",
    "    betas=(0.5, 0.9),\n",
    "    weight_decay=discriminator_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b71c87a8-6826-4ad0-887e-dd222e4b8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Distribution\n",
    "mean = torch.zeros(batch_size, embedding_dim, device=device)\n",
    "std = mean + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cbf37aa-3f07-4eb6-aefc-7364f8da5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = pd.DataFrame(columns=['epoch', 'Generator Loss', 'Discriminator Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d948647-6439-44ee-8ae0-9fbf55e5f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generator (0.00) | Discriminator (0.00):   0%|                                                 | 0/300 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "epoch_iterator = tqdm(range(epochs), disable=(not verbose))\n",
    "if verbose:\n",
    "    description = 'Generator ({gen:.2f}) | Discriminator ({dis:.2f})'\n",
    "    epoch_iterator.set_description(description.format(gen=0, dis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58480ea2-7546-4208-9e82-f9217fc75d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = max(len(train_data) // batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36a006e7-bef4-40bf-b382-3e7845507f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choice_prob_index(discrete_column_id):\n",
    "    probs = discrete_column_category_prob[discrete_column_id]  # probability of each categories in the selected discrete_column\n",
    "    r = np.expand_dims(np.random.rand(probs.shape[0]), axis=1)  # randomly select one category based on the PMF of the selected column\n",
    "    return (probs.cumsum(axis=1) > r).argmax(axis=1)\n",
    "\n",
    "def sample_condvec(batch):\n",
    "    if n_discrete_columns == 0:\n",
    "        return None\n",
    "    discrete_column_id = np.random.choice(np.arange(n_discrete_columns), batch)  # randomly select one discrete column\n",
    "\n",
    "    cond = np.zeros((batch, n_categories), dtype='float32')  # concatenation of mask vectors\n",
    "    mask = np.zeros((batch, n_discrete_columns), dtype='float32')\n",
    "    mask[np.arange(batch), discrete_column_id] = 1\n",
    "    category_id_in_col = random_choice_prob_index(discrete_column_id)  # the category chosen within the selected discrete column\n",
    "    category_id = discrete_column_cond_st[discrete_column_id] + category_id_in_col\n",
    "    cond[np.arange(batch), category_id] = 1  # set that value to one\n",
    "\n",
    "    return cond, mask, discrete_column_id, category_id_in_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4210bf29-d200-419d-b1bd-ca8b003be915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(data, n, col, opt):\n",
    "    if col is None:\n",
    "        idx = np.random.randint(len(data), size=n)\n",
    "        return data[idx]\n",
    "        \n",
    "    idx = []\n",
    "    for c, o in zip(col, opt):\n",
    "        idx.append(np.random.choice(rid_by_cat_cols[c][o]))  # sample one data with the corresponding condition uniformly\n",
    "\n",
    "    return data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3924fad3-ac9f-4f34-a411-b55a17c7e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1):\n",
    "    for _ in range(10):\n",
    "        transformed = F.gumbel_softmax(logits, tau=tau, hard=hard, eps=eps, dim=dim)\n",
    "        if not torch.isnan(transformed).any():\n",
    "            return transformed\n",
    "\n",
    "    raise ValueError('gumbel_softmax returning NaN.')\n",
    "\n",
    "def apply_activate(data):\n",
    "    data_t = []\n",
    "    st = 0\n",
    "    for column_info in output_info_list:\n",
    "        for span_info in column_info['output_info']:\n",
    "            if span_info.activation_fn == 'tanh':\n",
    "                ed = st + span_info.dim\n",
    "                data_t.append(torch.tanh(data[:, st:ed]))\n",
    "                st = ed\n",
    "            elif span_info.activation_fn == 'softmax':\n",
    "                ed = st + span_info.dim\n",
    "                transformed = gumbel_softmax(data[:, st:ed], tau=0.2)\n",
    "                data_t.append(transformed)\n",
    "                st = ed\n",
    "            else:\n",
    "                raise ValueError(f'Unexpected activation function {span_info.activation_fn}')\n",
    "\n",
    "    return torch.cat(data_t, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3edfa6c-b8e2-4647-9a40-9401dab727f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_loss(data, c, m):\n",
    "    loss = []\n",
    "    st = 0\n",
    "    st_c = 0\n",
    "    for column_info in output_info_list:\n",
    "        for span_info in column_info['output_info']:\n",
    "            if column_info['column_type'] != 'discrete':\n",
    "                # not discrete column\n",
    "                st += span_info.dim\n",
    "            else:\n",
    "                ed = st + span_info.dim\n",
    "                ed_c = st_c + span_info.dim\n",
    "                tmp = F.cross_entropy(\n",
    "                    data[:, st:ed], torch.argmax(c[:, st_c:ed_c], dim=1), reduction='none'\n",
    "                )\n",
    "                loss.append(tmp)\n",
    "                st = ed\n",
    "                st_c = ed_c\n",
    "\n",
    "    loss = torch.stack(loss, dim=1)\n",
    "\n",
    "    return (loss * m).sum() / data.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb7bd267-b0a3-417e-b42a-46a3e8ff3556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generator (-0.38) | Discriminator (-0.25): 100%|█████████████████████████████████████| 300/300 [01:02<00:00,  4.78it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in epoch_iterator:\n",
    "    for id_ in range(steps_per_epoch):\n",
    "        # discriminator\n",
    "        for n in range(discriminator_step):\n",
    "            fakez = torch.normal(mean=mean, std=std)  # from standard deviation\n",
    "\n",
    "            condvec = sample_condvec(batch_size)\n",
    "            if condvec is None:\n",
    "                c1, m1, col, opt = None, None, None, None\n",
    "                real = sample_data(train_data, batch_size, col, opt)\n",
    "\n",
    "            else:\n",
    "                c1, m1, col, opt = condvec\n",
    "                # c1: cond vector\n",
    "                # m1: mask\n",
    "                # col: randomly chosen discrete column id\n",
    "                # opt: randomly chosen category of the corresponding column_id\n",
    "                c1 = torch.from_numpy(c1).to(device)\n",
    "                m1 = torch.from_numpy(m1).to(device)\n",
    "                fakez = torch.cat([fakez, c1], dim=1)  # input z \\oplus cond\n",
    "\n",
    "                perm = np.arange(batch_size)  # permutation for shuffling\n",
    "                np.random.shuffle(perm)\n",
    "                real = sample_data(train_data, batch_size, col[perm], opt[perm])  # sampled data based on the given condition\n",
    "                c2 = c1[perm]\n",
    "\n",
    "            fake = generator(fakez)\n",
    "            fakeact = apply_activate(fake)\n",
    "\n",
    "            real = torch.from_numpy(real.astype('float32')).to(device)\n",
    "\n",
    "            # real and fake data pacs\n",
    "            if c1 is not None:\n",
    "                fake_cat = torch.cat([fakeact, c1], dim=1)\n",
    "                real_cat = torch.cat([real, c2], dim=1)\n",
    "            else:\n",
    "                real_cat = real\n",
    "                fake_cat = fakeact\n",
    "\n",
    "            y_fake = discriminator(fake_cat)\n",
    "            y_real = discriminator(real_cat)\n",
    "\n",
    "            pen = discriminator.calc_gradient_penalty(\n",
    "                real_cat, fake_cat, device, pac\n",
    "            )\n",
    "            loss_d = -(torch.mean(y_real) - torch.mean(y_fake))\n",
    "\n",
    "            optimizerD.zero_grad(set_to_none=False)\n",
    "            pen.backward(retain_graph=True)\n",
    "            loss_d.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "        # generator\n",
    "        fakez = torch.normal(mean=mean, std=std)\n",
    "        condvec = sample_condvec(batch_size)\n",
    "\n",
    "        if condvec is None:\n",
    "            c1, m1, col, opt = None, None, None, None\n",
    "        else:\n",
    "            c1, m1, col, opt = condvec\n",
    "            c1 = torch.from_numpy(c1).to(device)\n",
    "            m1 = torch.from_numpy(m1).to(device)\n",
    "            fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "        fake = generator(fakez)\n",
    "        fakeact = apply_activate(fake)\n",
    "\n",
    "        if c1 is not None:\n",
    "            y_fake = discriminator(torch.cat([fakeact, c1], dim=1))\n",
    "        else:\n",
    "            y_fake = discriminator(fakeact)\n",
    "\n",
    "        if condvec is None:\n",
    "            cross_entropy = 0\n",
    "        else:\n",
    "            cross_entropy = cond_loss(fake, c1, m1)\n",
    "\n",
    "        loss_g = -torch.mean(y_fake) + cross_entropy\n",
    "\n",
    "        optimizerG.zero_grad(set_to_none=False)\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "    generator_loss = loss_g.detach().cpu().item()\n",
    "    discriminator_loss = loss_d.detach().cpu().item()\n",
    "\n",
    "    epoch_loss_df = pd.DataFrame({\n",
    "        'Epoch': [i],\n",
    "        'Generator Loss': [generator_loss],\n",
    "        'Discriminator Loss': [discriminator_loss]\n",
    "    })\n",
    "    if not loss_values.empty:\n",
    "        loss_values = pd.concat([loss_values, epoch_loss_df]).reset_index(drop=True)\n",
    "    else:\n",
    "        loss_values = epoch_loss_df\n",
    "\n",
    "    if verbose:\n",
    "        epoch_iterator.set_description(\n",
    "            description.format(gen=generator_loss, dis=discriminator_loss)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "299bf0ec-7688-440e-9187-bd8f32f7f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(train_data)\n",
    "sample_condition_column = None\n",
    "sample_condition_value = None\n",
    "dataframe = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5b6635e-ae72-4e7f-bc5b-9113de05af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_name_value_to_id(column_name, value):\n",
    "    discrete_counter = 0\n",
    "    column_id = 0\n",
    "    for column_transform_info in output_info_list:\n",
    "        if column_transform_info['column_name'] == column_name:\n",
    "            break\n",
    "        if column_transform_info['column_type'] == 'discrete':\n",
    "            discrete_counter += 1\n",
    "\n",
    "        column_id += 1\n",
    "    else:\n",
    "        raise ValueError(f\"{column_name} doesn't exist in the data\")\n",
    "\n",
    "    ohe = column_transform_info['transform']\n",
    "    data = pd.DataFrame([value], columns=[column_transform_info['column_name']])\n",
    "    one_hot = ohe.transform(data).to_numpy()\n",
    "    if sum(one_hot) == 0:\n",
    "        raise ValueError(f\"The value `{value}` doesn't exist in the column `{column_name}`\")\n",
    "    return {\n",
    "        'discrete_column_id': discrete_counter,\n",
    "        'column_id': column_id,\n",
    "        'value_id': np.argmax(one_hot)\n",
    "    }\n",
    "\n",
    "def inverse_transform_continuous(column_transform_info, column_data, sigmas, st):\n",
    "    gm = column_transform_info['transform']\n",
    "    # data contains [normalized data, selected component index]\n",
    "    data = pd.DataFrame(column_data[:, :2]).astype(float)\n",
    "    data[data.columns[1]] = np.argmax(column_data[:, 1:], axis=1)\n",
    "    if sigmas is not None:\n",
    "        selected_normalized_value = np.random.normal(data.iloc[:, 0], sigmas[st])\n",
    "        data.iloc[:, 0] = selected_normalized_value\n",
    "\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        data = data.to_numpy()\n",
    "\n",
    "    normalized = np.clip(data[:, 0], -1, 1)  # normalized values\n",
    "    means = gm.means_.reshape([-1])\n",
    "    stds = np.sqrt(gm.covariances_).reshape([-1])\n",
    "    selected_component = data[:, 1].round().astype(int)\n",
    "    selected_component = selected_component.clip(0, column_transform_info['valid_component_indicator'].sum() - 1)\n",
    "    std_t = stds[column_transform_info['valid_component_indicator']][selected_component]\n",
    "    mean_t = means[column_transform_info['valid_component_indicator']][selected_component]\n",
    "    recovered_data = normalized * 0.4 * std_t + mean_t\n",
    "\n",
    "    return recovered_data\n",
    "\n",
    "def inverse_transform_discrete(column_transform_info, column_data):\n",
    "    ohe = column_transform_info['transform']\n",
    "    data = pd.DataFrame(column_data)\n",
    "\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        data = data.to_numpy()\n",
    "\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(-1, 1)\n",
    "\n",
    "    indices = np.argmax(data, axis=1)\n",
    "    result = pd.Series(indices).astype(int)\n",
    "\n",
    "    return result\n",
    "\n",
    "def inverse_transform(data, sigmas=None):\n",
    "    st = 0\n",
    "    recovered_column_data_list = []\n",
    "    column_names = []\n",
    "    for column_transform_info in output_info_list:\n",
    "        dim = column_transform_info['output_dimensions']\n",
    "        column_data = data[:, st:st+dim]\n",
    "        if column_transform_info['column_type'] == 'continuous':\n",
    "            recovered_column_data = inverse_transform_continuous(column_transform_info, column_data, sigmas, st)\n",
    "        else:\n",
    "            recovered_column_data = inverse_transform_discrete(column_transform_info, column_data)\n",
    "            \n",
    "        recovered_column_data_list.append(recovered_column_data)\n",
    "        column_names.append(column_transform_info['column_name'])\n",
    "        st += dim\n",
    "\n",
    "    recovered_data = np.column_stack(recovered_column_data_list)\n",
    "    recovered_data = pd.DataFrame(recovered_data, columns=column_names)\n",
    "    if not dataframe:\n",
    "        recovered_data = recovered_data.to_numpy()\n",
    "\n",
    "    return recovered_data\n",
    "\n",
    "def generate_cond_from_condition_column_info(condition_info, batch):\n",
    "    vec = np.zeros((batch, n_categories), dtype='float32')\n",
    "    id_ = discrete_column_matrix_st[condition_info['discrete_column_id']]\n",
    "    id_ += condition_info['value_id']\n",
    "    vec[:, id_] = 1\n",
    "    return vec\n",
    "\n",
    "def sample_original_condvec(batch):\n",
    "    if n_discrete_columns == 0:\n",
    "        return None\n",
    "\n",
    "    category_freq = discrete_column_category_prob.flatten()\n",
    "    category_freq = category_freq[category_freq != 0]\n",
    "    category_freq = category_freq / np.sum(category_freq)\n",
    "    col_idxs = np.random.choice(np.arange(len(category_freq)), batch, p=category_freq)\n",
    "    cond = np.zeros((batch, n_categories), dtype='float32')\n",
    "    cond[np.arange(batch), col_idxs] = 1\n",
    "\n",
    "    return cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6c2219f-65d7-41c0-8025-1e40ad17d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(n, condition_column=None, condition_value=None):\n",
    "    if condition_column is not None and condition_value is not None:\n",
    "        condition_info = convert_column_name_value_to_id(condition_column, condition_value)\n",
    "        global_condition_vec = generate_cond_from_condition_column_info(condition_info, batch_size)\n",
    "    else:\n",
    "        global_condition_vec = None\n",
    "\n",
    "    steps = n // batch_size + 1\n",
    "    data = []\n",
    "    for i in range(steps):\n",
    "        mean = torch.zeros(batch_size, embedding_dim)\n",
    "        std = mean + 1\n",
    "        fakez = torch.normal(mean=mean, std=std).to(device)\n",
    "\n",
    "        if global_condition_vec is not None:\n",
    "            condvec = global_condition_vec.copy()\n",
    "        else:\n",
    "            condvec = sample_original_condvec(batch_size)\n",
    "\n",
    "        if condvec is None:\n",
    "            pass\n",
    "        else:\n",
    "            c1 = condvec\n",
    "            c1 = torch.from_numpy(c1).to(device)\n",
    "            fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "        fake = generator(fakez)\n",
    "        fakeact = apply_activate(fake)\n",
    "        data.append(fakeact.detach().cpu().numpy())\n",
    "\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    data = data[:n]\n",
    "\n",
    "    return inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dca7ff56-40a9-4f20-b5fd-95e60d684d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = sample(num_samples, sample_condition_column, sample_condition_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "987e3964-99f7-4fce-a75e-fcc18141ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(sampled.columns):\n",
    "    sampled[col] = sampled[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b415dac3-b6f2-4f64-9874-500f8c61f32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>300591</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>337829</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>114462</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1827</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>51762</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>-2</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>52776</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>186657</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>56595</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>248664</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4313</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>52576</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>109981</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-27</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  workclass  fnlwgt  education  education-num  marital-status  \\\n",
       "0      57          1  300591          5              9               0   \n",
       "1      56          2  337829          3              9               2   \n",
       "2      49          1  114462          3              4               0   \n",
       "3      48          1   51762          5             14               1   \n",
       "4      22          1   52776          1              9               2   \n",
       "...   ...        ...     ...        ...            ...             ...   \n",
       "1995   38          0  186657          3              4               2   \n",
       "1996   55          1   56595          3             14               1   \n",
       "1997   57          1  248664          0              4               0   \n",
       "1998   23          5   52576          1              9               3   \n",
       "1999   23          5  109981          3             11               4   \n",
       "\n",
       "      occupation  relationship  race  sex  capital-gain  capital-loss  \\\n",
       "0              9             2     0    0             3             1   \n",
       "1              8             0     0    1            21             0   \n",
       "2              8             2     0    1             4          1827   \n",
       "3              5             3     0    0            27            -2   \n",
       "4              0             3     0    1            -2            -1   \n",
       "...          ...           ...   ...  ...           ...           ...   \n",
       "1995           9             2     0    1            14             0   \n",
       "1996           4             3     0    0            -5             0   \n",
       "1997           7             2     0    1          4313             0   \n",
       "1998           8             3     0    0            46             0   \n",
       "1999           3             3     4    0           -27             0   \n",
       "\n",
       "      hours-per-week  native-country  \n",
       "0                 35               1  \n",
       "1                 35               8  \n",
       "2                 50               1  \n",
       "3                 49               1  \n",
       "4                 49               1  \n",
       "...              ...             ...  \n",
       "1995              39              28  \n",
       "1996              35               1  \n",
       "1997              39               1  \n",
       "1998              39               1  \n",
       "1999              39               0  \n",
       "\n",
       "[2000 rows x 14 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
